{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKMX5oKnII49"
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import collections\n",
        "from sklearn import metrics\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v27ckn5VIq_I"
      },
      "source": [
        "# activation functions\n",
        "def logistic(x, derivative=False):\n",
        "    if derivative:\n",
        "        return (np.exp(-x)) / ((np.exp(-x) + 1) ** 2)\n",
        "    else:\n",
        "        return np.exp(x) / (np.exp(x) + 1)\n",
        "\n",
        "\n",
        "def leaky_relu(x, derivative=True):\n",
        "    if derivative:\n",
        "        dx = np.ones_like(x)\n",
        "        dx[x < 0] = .01\n",
        "        return dx\n",
        "    else:\n",
        "        return np.maximum(.01 * x, x)\n",
        "\n",
        "\n",
        "def relu(x, derivative=False):\n",
        "    if derivative:\n",
        "        return (x > 0).astype(int)\n",
        "    else:\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def softmax(x, derivative=False):\n",
        "    # subtract the max of X to make softmax stable. as suggested by the top answer here :\n",
        "    # https://stackoverflow.com/questions/61425412/stable-softmax-function-returns-wrong-output\n",
        "    exps = np.exp(x - x.max())\n",
        "    if derivative:\n",
        "        return exps / np.sum(exps, axis=0) * (1. - exps / np.sum(exps, axis=0))\n",
        "    else:\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "\n",
        "def tanh(x, derivative=False):\n",
        "  if derivative:\n",
        "    return 1.0-(np.tanh(x)**2)\n",
        "  else:\n",
        "    return np.tanh(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8VIeCnwJPEs"
      },
      "source": [
        "# vectorization function\n",
        "def vectorize(x):\n",
        "    # takes in  6000 [28x28] arrays. \"flattens\" them into 6000 [784x1] arrays 1 by 1.\n",
        "    temp = []\n",
        "    for i in range(x.shape[0]):\n",
        "        temp.append(x[i].flatten())\n",
        "\n",
        "    return np.array(temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k534ZmjVIy4Y",
        "outputId": "7b8149ba-02ce-4abd-aa52-f6aa7dc10290"
      },
      "source": [
        "# load data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R43a7uZjI3Ko"
      },
      "source": [
        "# normalize data\n",
        "x_train, x_test = x_train[..., np.newaxis] / 255.0, x_test[..., np.newaxis] / 255.0\n",
        "y_train, y_test = tf.keras.utils.to_categorical(y_train), tf.keras.utils.to_categorical(y_test)\n",
        "vectorized_x_train, vectorized_x_test = vectorize(x_train), vectorize(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0TKgGggQuYP"
      },
      "source": [
        "# Standardization\n",
        "\n",
        "#mean_test = vectorized_x_test.mean().astype(np.float32)\n",
        "#std_test = vectorized_x_test.std().astype(np.float32)\n",
        "#vectorized_x_test = (vectorized_x_test - mean_test)/(std_test)\n",
        "\n",
        "#mean_train = vectorized_x_train.mean().astype(np.float32)\n",
        "#std_train = vectorized_x_train.std().astype(np.float32)\n",
        "#vectorized_x_train = (vectorized_x_train - mean_train)/(std_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O_IenzZI9ch"
      },
      "source": [
        "class MultilayerPerceptron:\n",
        "\n",
        "    def __init__(self, actFunction, nbOfLayers, nbOfUnits):\n",
        "\n",
        "        # check that the input is valid\n",
        "        assert len(nbOfUnits) == nbOfLayers\n",
        "        self.actFunction = actFunction\n",
        "        self.nbOfLayers = nbOfLayers\n",
        "        self.nbOfUnits = nbOfUnits\n",
        "        # temporarly initate lr to 0.01. will be overwritten with the desired Value in fit\n",
        "        self.lr = 0.01\n",
        "        # create a list that contains all layers and the number of neurons in them. the input and output layers are\n",
        "        # hard coded for the MINST Digit data set. they would have been parameters if the Assigenment didn't limit\n",
        "        # parameters to actFunction, nbOfLayers, nbOfUnits\n",
        "        self.layers = [28 * 28] + nbOfUnits + [10]\n",
        "\n",
        "        # initiate the weight and bias matrices with random values. np.random.randn gave me a better initial result\n",
        "        # compared with rand. Uses Xavier initaliztion for weights http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
        "        self.params = {}\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.params[f'W{i + 1}'] = np.random.randn(self.layers[i + 1], self.layers[i]) * 1 / np.sqrt(self.layers[i])\n",
        "            self.params[f'B{i + 1}'] = np.zeros(self.layers[i + 1])\n",
        "\n",
        "    def forward_propogate(self, inputs):\n",
        "        # activation for the input layer is just the input\n",
        "        self.params['A0'] = inputs\n",
        "        # compute the activation of each layer. Z is the net input and A is the activation\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.params[f'Z{i + 1}'] = np.dot(self.params[f'W{i + 1}'], self.params[f'A{i}']) + self.params[f'B{i + 1}']\n",
        "            if i != len(self.layers) - 1:\n",
        "                self.params[f'A{i + 1}'] = self.actFunction(self.params[f'Z{i + 1}'])\n",
        "            else:\n",
        "                # last layer uses softmax\n",
        "                self.params[f'A{i + 1}'] = softmax(self.params[f'Z{i + 1}'])\n",
        "\n",
        "        # return the activation of the output layer(the \"prediction\").\n",
        "        return self.params[f'A{len(self.layers) - 1}']\n",
        "\n",
        "    def backwards_propagate(self, y_train, network_output, L2):\n",
        "        N = network_output.shape\n",
        "        # compute the error for the the output layer. divide by N to correct dimensions\n",
        "        error = (network_output - y_train) * softmax(self.params[f'Z{len(self.layers) - 1}'], True) / N\n",
        "\n",
        "        # traverse through the layers in reverse. from the last layer , [len - 1 ] , to the first hidden layer.\n",
        "        # uses range ( start, end , step) function.\n",
        "\n",
        "        for i in range(len(self.layers) - 1, 0, -1):\n",
        "            # output, hidden , input\n",
        "            # modify the weight based on the learning rate\n",
        "            self.params[f'W{i}'] -= self.lr * ((np.outer(error, self.params[f'A{i - 1}'])) + L2 * self.params[f'W{i}'])\n",
        "            self.params[f'B{i}'] -= self.lr * np.sum(error, axis=0)/N\n",
        "            if i!=1:\n",
        "                # self.params[W0] doesn't exist.so skip the i = 1 case. computes the error for the weight adjustment\n",
        "                # of the next layer\n",
        "                error = self.params[f'W{i}'].transpose().dot(error) * self.actFunction(self.params[f'Z{i - 1}'],\n",
        "                                                                                       derivative=True)\n",
        "        return self\n",
        "\n",
        "    def fit(self, x, y, x_test, y_test, lr, iterations, variable_lr,L2 = 0.0, variable_duration=10, min_lr=0.001):\n",
        "        # overwrite the default learning rate\n",
        "        self.lr = lr\n",
        "        predictions = []\n",
        "        epochs = []\n",
        "        training_accuracies = []\n",
        "        testing_accuracies = []\n",
        "        lr_counter = 0\n",
        "        for i in range(iterations):\n",
        "            # go through each example in the training set. record the predictions to compute the accuracy of the\n",
        "            # training set\n",
        "            for X, Y in zip(x, y):\n",
        "                output = self.forward_propogate(X)\n",
        "                prediction = np.argmax(output)\n",
        "                label = np.argmax(Y)\n",
        "                predictions.append(prediction == label)\n",
        "                self.backwards_propagate(Y, output, L2)\n",
        "\n",
        "            if variable_lr:\n",
        "                self.lr = max(self.lr / (1 + int((lr_counter / variable_duration))), min_lr)\n",
        "                lr_counter = lr_counter + 1\n",
        "                if lr_counter > variable_duration:\n",
        "                    lr_counter = 0\n",
        "            \n",
        "\n",
        "            # compute accuracy on the training and testing set to ensure learning and detect over-fitting and divergence\n",
        "            epochs.append(i)\n",
        "            a = self.accuracy(x_test, y_test)\n",
        "            testing_accuracies.append(round(a * 100, 1))\n",
        "            b = np.mean(predictions)\n",
        "            training_accuracies.append(round(b * 100, 1))\n",
        "           \n",
        "            print(f'{i} iteration : test accuracy {round(a * 100, 1)}%, training accuracy  {round(b * 100, 1)}%. learning at {self.lr}')\n",
        "\n",
        "        # Plotting testing and training accuracies     \n",
        "        plt.plot(epochs, training_accuracies, 'g', label='Training Accuracy')\n",
        "        plt.plot(epochs, testing_accuracies, 'b', label='Testing Accuracy')\n",
        "        plt.title('Training and Testing accuracies vs Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def accuracy(self, x_test, y_test):\n",
        "        predictions = []\n",
        "        # compute accuracy by comparing the results of a forward propagate and the true label.\n",
        "        # np.argmax was used to de-categorize\n",
        "        for X, Y in zip(x_test, y_test):\n",
        "            pre = self.predict(X)\n",
        "            prediction = np.argmax(pre)\n",
        "            label = np.argmax(Y)\n",
        "            predictions.append(prediction == label)\n",
        "\n",
        "        return np.array(predictions).mean()\n",
        "\n",
        "    def getPredictions(self, x_test, y_test):\n",
        "      # gets a list of the the predictions\n",
        "      predictions = []\n",
        "      for X, Y in zip(x_test, y_test):\n",
        "            pre = self.predict(X)\n",
        "            prediction = np.argmax(pre)\n",
        "            label = np.argmax(Y)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "      return np.array(predictions)\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.forward_propogate(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKbWiYpeJX6c"
      },
      "source": [
        "Expirement 1. Network Arch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guCeQ2YDJbUq"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 2, [32,32])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bHJiwwlGXAoK"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 2, [64,64])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OpySGMelXJ1i"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uueMGsF_p_N"
      },
      "source": [
        "*Expirement* 2. # hidden layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vXvtI_xK_uM7"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 0, [])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bg-qnD7T_2T9"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 1, [128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9TaOvguQ_58e"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YswNhHhB_8vT"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 3, [128,128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4vUe66iAA-7"
      },
      "source": [
        "Expirement 3. Activation functions\n",
        "NOTE: we found the optimal learning rate to be 0.9 for logistic and 0.01 for Relu/tanh "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKNtgb01AMoh"
      },
      "source": [
        "mlp = MultilayerPerceptron(tanh, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj44ODqKAKt3"
      },
      "source": [
        "mlp = MultilayerPerceptron(logistic, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.9, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSzPEK67AJSj"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-7G5Y3GAXA8"
      },
      "source": [
        "Expirement 4. Variable Learning Rate\n",
        "\n",
        "Halves the learning rate every N itterations. set to 10 by default\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYzjo-7XAUmC"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=250, variable_lr=True, variable_duration=10 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BAMBHYVA0Ev"
      },
      "source": [
        "Expirement 5. L2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFdcWhieAzkg"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False, L2 = 0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YvYFrEaBH_S"
      },
      "source": [
        "Expirement 6. Xavier Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5v8Y_RUBMw7"
      },
      "source": [
        "With Xavier Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dap-CAFjBPOF"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tgwwCllCYmmW"
      },
      "source": [
        "mlp = MultilayerPerceptron(logistic, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.9, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syrGPuOABWFF"
      },
      "source": [
        "Without Xavier Initilization(below is an MLP Class without Xavier Initilization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLZspRBABa7g"
      },
      "source": [
        "class MultilayerPerceptron2:\n",
        "\n",
        "    def __init__(self, actFunction, nbOfLayers, nbOfUnits):\n",
        "\n",
        "        # check that the input is valid\n",
        "        assert len(nbOfUnits) == nbOfLayers\n",
        "        self.actFunction = actFunction\n",
        "        self.nbOfLayers = nbOfLayers\n",
        "        self.nbOfUnits = nbOfUnits\n",
        "        # temporarly initate lr to 0.01. will be overwritten with the desired Value in fit\n",
        "        self.lr = 0.01\n",
        "        # create a list that contains all layers and the number of neurons in them. the input and output layers are\n",
        "        # hard coded for the MINST Digit data set. they would have been parameters if the Assigenment didn't limit\n",
        "        # parameters to actFunction, nbOfLayers, nbOfUnits\n",
        "        self.layers = [28 * 28] + nbOfUnits + [10]\n",
        "\n",
        "        # initiate the weight and bias matrices with random values. np.random.randn gave me a better initial result\n",
        "        # compared with rand. Uses Xavier initaliztion for weights http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
        "        self.params = {}\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.params[f'W{i + 1}'] = np.random.randn(self.layers[i + 1], self.layers[i])\n",
        "            self.params[f'B{i + 1}'] = np.zeros(self.layers[i + 1])\n",
        "\n",
        "    def forward_propogate(self, inputs):\n",
        "        # activation for the input layer is just the input\n",
        "        self.params['A0'] = inputs\n",
        "        # compute the activation of each layer. Z is the net input and A is the activation\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.params[f'Z{i + 1}'] = np.dot(self.params[f'W{i + 1}'], self.params[f'A{i}']) + self.params[f'B{i + 1}']\n",
        "            if i != len(self.layers) - 1:\n",
        "                self.params[f'A{i + 1}'] = self.actFunction(self.params[f'Z{i + 1}'])\n",
        "            else:\n",
        "                # last layer uses softmax\n",
        "                self.params[f'A{i + 1}'] = softmax(self.params[f'Z{i + 1}'])\n",
        "\n",
        "        # return the activation of the output layer(the \"prediction\").\n",
        "        return self.params[f'A{len(self.layers) - 1}']\n",
        "\n",
        "    def backwards_propagate(self, y_train, network_output, L2):\n",
        "        N = network_output.shape\n",
        "        # compute the error for the the output layer. divide by N to correct dimensions\n",
        "        error = (network_output - y_train) * softmax(self.params[f'Z{len(self.layers) - 1}'], True) / N\n",
        "\n",
        "        # traverse through the layers in reverse. from the last layer , [len - 1 ] , to the first hidden layer.\n",
        "        # uses range ( start, end , step) function.\n",
        "\n",
        "        for i in range(len(self.layers) - 1, 0, -1):\n",
        "            # output, hidden , input\n",
        "            # modify the weight based on the learning rate\n",
        "            self.params[f'W{i}'] -= self.lr * ((np.outer(error, self.params[f'A{i - 1}'])) + L2 * self.params[f'W{i}'])\n",
        "            self.params[f'B{i}'] -= self.lr * np.sum(error, axis=0)/N\n",
        "            if i!=1:\n",
        "                # self.params[W0] doesn't exist.so skip the i = 1 case. computes the error for the weight adjustment\n",
        "                # of the next layer\n",
        "                error = self.params[f'W{i}'].transpose().dot(error) * self.actFunction(self.params[f'Z{i - 1}'],\n",
        "                                                                                       derivative=True)\n",
        "        return self\n",
        "\n",
        "    def fit(self, x, y, x_test, y_test, lr, iterations, variable_lr,L2 = 0.0, variable_duration=10, min_lr=0.001):\n",
        "        # overwrite the default learning rate\n",
        "        self.lr = lr\n",
        "        predictions = []\n",
        "        epochs = []\n",
        "        training_accuracies = []\n",
        "        testing_accuracies = []\n",
        "        lr_counter = 0\n",
        "        for i in range(iterations):\n",
        "            # go through each example in the training set. record the predictions to compute the accuracy of the\n",
        "            # training set\n",
        "            for X, Y in zip(x, y):\n",
        "                output = self.forward_propogate(X)\n",
        "                prediction = np.argmax(output)\n",
        "                label = np.argmax(Y)\n",
        "                predictions.append(prediction == label)\n",
        "                self.backwards_propagate(Y, output, L2)\n",
        "\n",
        "            if variable_lr:\n",
        "                self.lr = max(self.lr / (1 + int((lr_counter / variable_duration))), min_lr)\n",
        "                lr_counter = lr_counter + 1\n",
        "                if lr_counter > variable_duration:\n",
        "                    lr_counter = 0\n",
        "            \n",
        "\n",
        "            # compute accuracy on the training and testing set to ensure learning and detect over-fitting and divergence\n",
        "            epochs.append(i)\n",
        "            a = self.accuracy(x_test, y_test)\n",
        "            testing_accuracies.append(round(a * 100, 1))\n",
        "            b = np.mean(predictions)\n",
        "            training_accuracies.append(round(b * 100, 1))\n",
        "           \n",
        "            print(f'{i} iteration : test accuracy {round(a * 100, 1)}%, training accuracy  {round(b * 100, 1)}%. learning at {self.lr}')\n",
        "\n",
        "        # Plotting testing and training accuracies     \n",
        "        plt.plot(epochs, training_accuracies, 'g', label='Training Accuracy')\n",
        "        plt.plot(epochs, testing_accuracies, 'b', label='Testing Accuracy')\n",
        "        plt.title('Training and Testing accuracies vs Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def accuracy(self, x_test, y_test):\n",
        "        predictions = []\n",
        "        # compute accuracy by comparing the results of a forward propagate and the true label.\n",
        "        # np.argmax was used to de-categorize\n",
        "        for X, Y in zip(x_test, y_test):\n",
        "            pre = self.predict(X)\n",
        "            prediction = np.argmax(pre)\n",
        "            label = np.argmax(Y)\n",
        "            predictions.append(prediction == label)\n",
        "\n",
        "        return np.array(predictions).mean()\n",
        "\n",
        "    def getPredictions(self, x_test, y_test):\n",
        "      # gets a list of the the predictions\n",
        "      predictions = []\n",
        "      for X, Y in zip(x_test, y_test):\n",
        "            pre = self.predict(X)\n",
        "            prediction = np.argmax(pre)\n",
        "            label = np.argmax(Y)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "      return np.array(predictions)\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.forward_propogate(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9hiQzqRBqTH"
      },
      "source": [
        "mlp = MultilayerPerceptron2(relu, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-o8laC7YseW"
      },
      "source": [
        "mlp = MultilayerPerceptron2(logistic, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.9, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNoj_ARhDi0C"
      },
      "source": [
        "Expirement 7. Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KpU8ifWBxaI"
      },
      "source": [
        "# Comment out standardization for this test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XWYrbceBxyd"
      },
      "source": [
        "Expirement 8. Bias Term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3xECsnyELyL"
      },
      "source": [
        "With Bias Term"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZNic384B1I-"
      },
      "source": [
        "mlp = MultilayerPerceptron(relu, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ntEAlgOEN90"
      },
      "source": [
        "Without Bias Term(Below is an MLP Class without the bias term being included in the activation or being updated)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p08Nfc2DB4N8"
      },
      "source": [
        "class MultilayerPerceptron3:\n",
        "\n",
        "    def __init__(self, actFunction, nbOfLayers, nbOfUnits):\n",
        "\n",
        "        # check that the input is valid\n",
        "        assert len(nbOfUnits) == nbOfLayers\n",
        "        self.actFunction = actFunction\n",
        "        self.nbOfLayers = nbOfLayers\n",
        "        self.nbOfUnits = nbOfUnits\n",
        "        # temporarly initate lr to 0.01. will be overwritten with the desired Value in fit\n",
        "        self.lr = 0.01\n",
        "        # create a list that contains all layers and the number of neurons in them. the input and output layers are\n",
        "        # hard coded for the MINST Digit data set. they would have been parameters if the Assigenment didn't limit\n",
        "        # parameters to actFunction, nbOfLayers, nbOfUnits\n",
        "        self.layers = [28 * 28] + nbOfUnits + [10]\n",
        "\n",
        "        # initiate the weight and bias matrices with random values. np.random.randn gave me a better initial result\n",
        "        # compared with rand\n",
        "        self.params = {}\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.params[f'W{i + 1}'] = np.random.randn(self.layers[i + 1], self.layers[i]) * 1 / np.sqrt(self.layers[i])\n",
        "            self.params[f'B{i + 1}'] = np.zeros(self.layers[i + 1])\n",
        "\n",
        "    def forward_propogate(self, inputs):\n",
        "        # activation for the input layer is just the input\n",
        "        self.params['A0'] = inputs\n",
        "        # compute the activation of each layer. Z is the net input and A is the activation\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.params[f'Z{i + 1}'] = np.dot(self.params[f'W{i + 1}'], self.params[f'A{i}']) \n",
        "            if i != len(self.layers) - 1:\n",
        "                self.params[f'A{i + 1}'] = self.actFunction(self.params[f'Z{i + 1}'])\n",
        "            else:\n",
        "                # last layer uses softmax\n",
        "                self.params[f'A{i + 1}'] = softmax(self.params[f'Z{i + 1}'])\n",
        "\n",
        "        # return the activation of the output layer(the \"prediction\").\n",
        "        return self.params[f'A{len(self.layers) - 1}']\n",
        "\n",
        "    def backwards_propagate(self, y_train, network_output, L2):\n",
        "        N = network_output.shape\n",
        "        # compute the error for the the output layer. divide by N to correct dimensions\n",
        "        error = (network_output - y_train) * softmax(self.params[f'Z{len(self.layers) - 1}'], True) / N\n",
        "\n",
        "        # traverse through the layers in reverse. from the last layer , [len - 1 ] , to the first hidden layer.\n",
        "        # uses range ( start, end , step) function.\n",
        "\n",
        "        for i in range(len(self.layers) - 1, 0, -1):\n",
        "            # output, hidden , input\n",
        "            # modify the weight based on the learning rate\n",
        "            self.params[f'W{i}'] -= self.lr * ((np.outer(error, self.params[f'A{i - 1}'])) + L2 * self.params[f'W{i}'])\n",
        "            if i!=1:\n",
        "                # self.params[W0] doesn't exist.so skip the i = 1 case. computes the error for the weight adjustment\n",
        "                # of the next layer\n",
        "                error = self.params[f'W{i}'].transpose().dot(error) * self.actFunction(self.params[f'Z{i - 1}'],\n",
        "                                                                                       derivative=True)\n",
        "        return self\n",
        "\n",
        "    def fit(self, x, y, x_test, y_test, lr, iterations, variable_lr,L2 = 0.0, variable_duration=10, min_lr=0.001):\n",
        "        # overwrite the default learning rate\n",
        "        self.lr = lr\n",
        "        predictions = []\n",
        "        epochs = []\n",
        "        training_accuracies = []\n",
        "        testing_accuracies = []\n",
        "        lr_counter = 0\n",
        "        for i in range(iterations):\n",
        "            # go through each example in the training set. record the predictions to compute the accuracy of the\n",
        "            # training set\n",
        "            for X, Y in zip(x, y):\n",
        "                output = self.forward_propogate(X)\n",
        "                prediction = np.argmax(output)\n",
        "                label = np.argmax(Y)\n",
        "                predictions.append(prediction == label)\n",
        "                self.backwards_propagate(Y, output, L2)\n",
        "\n",
        "            if variable_lr:\n",
        "                self.lr = max(self.lr / (1 + int((lr_counter / variable_duration))), min_lr)\n",
        "                lr_counter = lr_counter + 1\n",
        "                if lr_counter > variable_duration:\n",
        "                    lr_counter = 0\n",
        "            \n",
        "\n",
        "            # compute accuracy on the training and testing set to ensure learning and detect over-fitting and divergence\n",
        "            epochs.append(i)\n",
        "            a = self.accuracy(x_test, y_test)\n",
        "            testing_accuracies.append(round(a * 100, 1))\n",
        "            b = np.mean(predictions)\n",
        "            training_accuracies.append(round(b * 100, 1))\n",
        "           \n",
        "            print(f'{i} iteration : test accuracy {round(a * 100, 1)}%, training accuracy  {round(b * 100, 1)}%. learning at {self.lr}')\n",
        "\n",
        "        # Plotting testing and training accuracies     \n",
        "        plt.plot(epochs, training_accuracies, 'g', label='Training Accuracy')\n",
        "        plt.plot(epochs, testing_accuracies, 'b', label='Testing Accuracy')\n",
        "        plt.title('Training and Testing accuracies vs Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def accuracy(self, x_test, y_test):\n",
        "        predictions = []\n",
        "        # compute accuracy by comparing the results of a forward propagate and the true label.\n",
        "        # np.argmax was used to de-categorize\n",
        "        for X, Y in zip(x_test, y_test):\n",
        "            pre = self.predict(X)\n",
        "            prediction = np.argmax(pre)\n",
        "            label = np.argmax(Y)\n",
        "            predictions.append(prediction == label)\n",
        "\n",
        "        return np.array(predictions).mean()\n",
        "\n",
        "    def getPredictions(self, x_test, y_test):\n",
        "      # gets a list of the the predictions\n",
        "      predictions = []\n",
        "      for X, Y in zip(x_test, y_test):\n",
        "            pre = self.predict(X)\n",
        "            prediction = np.argmax(pre)\n",
        "            label = np.argmax(Y)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "      return np.array(predictions)\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.forward_propogate(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-adqIPMfCELG"
      },
      "source": [
        "mlp = MultilayerPerceptron3(relu, 2, [128,128])\n",
        "mlp.fit(vectorized_x_train, y_train, vectorized_x_test, y_test, lr=0.01, iterations=50, variable_lr=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}